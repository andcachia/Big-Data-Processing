{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 48.22900390625,
      "end_time": 1558871372239.283
     }
    }
   },
   "source": [
    "# SciPi Spark Implementation - Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joseph Azzopardi & Andrew Cachia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 51.455078125,
      "end_time": 1558871572373.696
     }
    }
   },
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 263.0869140625,
      "end_time": 1558871572948.064
     }
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.sql.functions as fn\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import os\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkMasterURL = \"spark://ubuntu:7077\"\n",
    "appName = 'SciPi'\n",
    "exeMemory = \"2g\"\n",
    "driMemory = \"1g\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7efc241a5470>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#findspark.init()\n",
    "    \n",
    "conf = SparkConf()\n",
    "    \n",
    "conf.setMaster(sparkMasterURL)\n",
    "conf.setAppName(appName)\n",
    "conf.set(\"spark.executor.memory\", exeMemory)\n",
    "conf.set(\"spark.driver.memory\", driMemory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start spark cluster\n",
    "\n",
    "#sc = pyspark.SparkContext(conf=conf)\n",
    "sc = pyspark.SparkContext()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author-Author Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 48.6298828125,
      "end_time": 1558871574676.244
     }
    }
   },
   "outputs": [],
   "source": [
    "#function to load the dataset from file\n",
    "\n",
    "def _1_load_data(full_path_to_file):\n",
    "    print (\"Input file location:\\t\", full_path_to_file)\n",
    "    return sc.textFile(full_path_to_file)\n",
    "\n",
    "def _2_authorAuthor_rel(distData, full_output_path, cSize=1):\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        #Split columns (tab delimited), filter columns, and convert to DataFrame\n",
    "        distData  = distData.map(lambda a: a.split(\"\\t\")).map(lambda x: (int(x[0]), int(x[1])) ).toDF([\"paperid\", \"authorid\"])\n",
    "\n",
    "        # inner join and use id. Since the paperid is repeated for every author instance an inner join is sufficient to create \n",
    "        # the permutations\n",
    "        df1 = distData.selectExpr(\"paperid as paperid_1\", \"authorid as authorid_1\").alias(\"df1\")\n",
    "        df2 = distData.selectExpr(\"paperid as paperid_2\", \"authorid as authorid_2\").alias(\"df2\")\n",
    "\n",
    "        distData = df1.join(df2, col(\"df1.paperid_1\") == col(\"df2.paperid_2\"), 'inner')\n",
    "\n",
    "        # the inner join create relationships between author, so they are filtered out.\n",
    "        distData = distData.select(\"authorid_1\", \"authorid_2\").filter(col(\"authorid_1\") != col(\"authorid_2\"))\n",
    "\n",
    "        # create new column that joins authorid_1 and authorid_2 using ':' as seperator, \n",
    "        # Keep new relations column only and add column with value of '1'\n",
    "\n",
    "        distData = distData.withColumn(\"author_rel\", concat(col(\"authorid_1\"), lit(\":\"), col(\"authorid_2\"))).select(\"author_rel\").withColumn(\"count\", lit(1))\n",
    "\n",
    "        # group rows by author_id (1st column), and find total number of collaboration per pair.\n",
    "        distData = distData.groupby(['author_rel']).agg(sum('count')).withColumn(\"split_rel\", split(distData.author_rel, ':'))\n",
    "        distData = distData.select(\"sum(count)\", \"split_rel\").select(col(\"sum(count)\").alias(\"collaborations\"), col(\"split_rel\").alias(\"author_rel\"))\n",
    "\n",
    "        # remove less interesting collaborations, and also to reduce output size\n",
    "        distData = distData.filter(distData.collaborations > cSize)\n",
    "\n",
    "        # Output to CSV - authorid_1, authorid_2, collaborations (frequency)\n",
    "        distData.select( distData.author_rel[0], distData.author_rel[1], distData.collaborations).write.csv(full_output_path)\n",
    "        \n",
    "        print (\"\\n<Author-Author Relationships> successfully processed.\\nFile output at:\\t\", full_output_path )\n",
    "        \n",
    "        # Clear DataFrame\n",
    "        pubData.unpersist()\n",
    "    \n",
    "    except Exception as e:\n",
    "        \n",
    "        print (\"An ERROR has occured while processing <Author-Author Relationships>.\")\n",
    "        print (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 2647291.4729003906,
      "end_time": 1558874224005.613
     }
    }
   },
   "outputs": [],
   "source": [
    "# Azure Storage Locations for input/output of Spark scripts.\n",
    "\n",
    "AzStorage_input_path  = \"wasbs://mag-2019-03-22@ics5114mag.blob.core.windows.net/mag/\"\n",
    "AzStorage_input_filename = \"PaperAuthorAffiliations.txt\"\n",
    "#AzStorage_input_path  = \"/home/data/input/\"\n",
    "#AzStorage_input_filename = \"PaperAuthorAffiliations_sample_10.txt\"\n",
    "AzStorage_input_location = AzStorage_input_path + AzStorage_input_filename\n",
    "\n",
    "\n",
    "AzStorage_output_path   = \"wasbs://parsed-csv-files@ics5114mag.blob.core.windows.net/results/\"\n",
    "#AzStorage_output_path   = \"/home/data/results/\"\n",
    "AzStorage_output_folder = \"author-author-rel\"\n",
    "AzStorage_output_location = AzStorage_output_path + AzStorage_output_folder\n",
    "\n",
    "distData = _1_load_data(AzStorage_input_location)\n",
    "\n",
    "# Create Author-Author Relationships\n",
    "# set Threshold on for smaller of equal collaborations to be discarded.\n",
    "collaborationIgnore = 1\n",
    "_2_authorAuthor_rel(distData, AzStorage_output_location, collaborationIgnore)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author-Paper-Rel-Institutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 249.02001953125,
      "end_time": 1558793690760.814
     }
    }
   },
   "outputs": [],
   "source": [
    "# DATASET --> { 0:PaperId, 1:AuthorId, 2:AffiliationId, 3:AuthorSequenceNumber, \n",
    "#               4:OriginalAuthor, 5:OriginalAffiliation }\n",
    "\n",
    "def _2_paperAuthorRel(distData, full_output_path):\n",
    "    \n",
    "    try:\n",
    "        # Split columns (tab delimited), filter required columns\n",
    "        distData = distData.map(lambda x: x.split(\"\\t\")).map(lambda x: (int(x[0]), int(x[1]), int(x[3]), str(x[4])) )\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        distData = distData.toDF([\"paperId\", \"authorId\", \"relationType\", \"institution\"])\n",
    "\n",
    "        # Mark Authors \"A\", and CoAuthors \"Co_A\". \n",
    "        # Keep labels short in order to reduce data required to store and process data.\n",
    "        distData = distData.withColumn('mRelationType', when(distData.relationType == 1, \"A\").otherwise(\"Co_A\"))\n",
    "        \n",
    "        # Output to CSV - PaperID, authorId, and relationship type.\n",
    "        distData.select(distData.paperId, distData.authorId, distData.mRelationType).write.csv(full_output_path)\n",
    "        \n",
    "        print (\"<Paper-Author Relationships> successfully processed.\\nFile output at:\\t\", full_output_path )\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        print (\"An ERROR has occured while processing <Paper-Author Relationships>.\")\n",
    "    \n",
    "    \n",
    "    return (distData)\n",
    "\n",
    "\n",
    "def _3_institutionsNodes(distData, full_output_path):\n",
    "    \n",
    "    try:\n",
    "        # Keep only required fields\n",
    "        distData = distData.select( distData.authorId, distData.institution)\n",
    "        \n",
    "        # Remove entries that have an empty institution Id\n",
    "        distData = distData.filter( distData.institution != \"\")\n",
    "        \n",
    "        # Create new dataframe from institution column and drop duplicate names\n",
    "        insData = distData.select(distData.institution).dropDuplicates([\"institution\"])\n",
    "        \n",
    "        # Assign an id to each institution\n",
    "        insData = insData.withColumn(\"institutionId\", monotonically_increasing_id())\n",
    "    \n",
    "        # Output to CSV - institutionId, and institution name.\n",
    "        insData.select(insData.institutionId, insData.institution).write.csv(full_output_path)\n",
    "        \n",
    "        print (\"<Institution Nodes> successfully processed.\\nFile output at:\\t\", full_output_path )\n",
    "    \n",
    "    except:\n",
    "        print (\"An ERROR has occured while processing <Institutions Nodes> \")\n",
    "    \n",
    "    \n",
    "    return (distData, insData)\n",
    "    \n",
    "    \n",
    "\n",
    "def _4_institutionsAuthorRel(distData, insData, full_output_path):\n",
    "    \n",
    "    try:\n",
    "        #Change Dataframe Headers and assign alias to dataframe\n",
    "        df1 = distData.selectExpr(\"authorid as authorId\", \"institution as institution_1\").alias(\"df1\")\n",
    "        df2 = insData.selectExpr(\"institution as institution_2\", \"institutionId as institutionId\").alias(\"df2\")\n",
    "\n",
    "        # Left Join dataframes so that we include the institutionId with the dataset\n",
    "        distData = df1.join(df2, col(\"df1.institution_1\") == col(\"df2.institution_2\"), 'left')\n",
    "        \n",
    "        # Drop duplicate names\n",
    "        distData = distData.drop(\"institution_1\").drop(\"institution_2\")\n",
    "        distData = distData.dropDuplicates([\"authorId\", \"institutionId\"])\n",
    "        \n",
    "        # Output to CSV - institutionId, and authorId.\n",
    "        distData.select(distData.authorId, distData.institutionId).write.csv(full_output_path)\n",
    "        \n",
    "        print (\"<Author-Institutions Relationships> successfully processed.\\nFile output at:\\t\", full_output_path )\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        print (\"An ERROR has occured while processing <Author-Institutions Relationships>.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 248.635009765625,
      "end_time": 1558793696645.631
     }
    }
   },
   "outputs": [],
   "source": [
    "#AzStorage_input_path  = \"wasbs://mag-2019-03-22@ics5114mag.blob.core.windows.net/mag/\"\n",
    "#AzStorage_input_filename = \"PaperAuthorAffiliations.txt\"\n",
    "AzStorage_input_path  = \"/home/data/input/\"\n",
    "AzStorage_input_filename = \"PaperAuthorAffiliations_sample_10.txt\"\n",
    "AzStorage_input_location = AzStorage_input_path + AzStorage_input_filename\n",
    "\n",
    "#AzStorage_output_path    = \"wasbs://parsed-csv-files@ics5114mag.blob.core.windows.net/results/\"\n",
    "AzStorage_output_path   = \"/home/data/results/\"\n",
    "\n",
    "AzStorage_output_folder_papAuthRel = \"papers-author-rel\"\n",
    "AzStorage_output_location_papAuthRel = AzStorage_output_path + AzStorage_output_folder_papAuthRel\n",
    "\n",
    "AzStorage_output_folder_institutions = \"institutions\"\n",
    "AzStorage_output_location_institutions = AzStorage_output_path + AzStorage_output_folder_institutions\n",
    "\n",
    "AzStorage_output_folder_authInstRel = \"author-institution-rel\"\n",
    "AzStorage_output_location_authInstRel = AzStorage_output_path + AzStorage_output_folder_authInstRel\n",
    "\n",
    "distData = _1_load_data(AzStorage_input_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 417363.2922363281,
      "end_time": 1558794128721.822
     }
    }
   },
   "outputs": [],
   "source": [
    "# Extract Paper-Author Relationships\n",
    "distData = _2_paperAuthorRel(distData, AzStorage_output_location_papAuthRel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Institution Nodes\n",
    "distData, insData = _3_institutionsNodes(distData, AzStorage_output_location_institutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Author-Institution Relationships\n",
    "_4_institutionsAuthorRel(distData, insData, AzStorage_output_location_authInstRel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 46.0009765625,
      "end_time": 1558784553367.894
     }
    }
   },
   "outputs": [],
   "source": [
    "# DATASET --> { 0:AuthorId, 1:Rank, 2:NormalizedName, 3:Display Name, 4:LastKnownAffiliationId, 5:PaperCount\n",
    "#               6:CitationCount, 7:CreateDate }\n",
    "\n",
    "def _2_author_nodes(distData, full_output_path):\n",
    "\n",
    "    try:\n",
    "        # Split columns (tab delimited), filter required columns\n",
    "        distData = distData.map(lambda x: x.split(\"\\t\")).map(lambda x: (int(x[0]), str(x[3]), int(x[5]), int(x[6]) ))\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        distData = distData.toDF([\"authorId\", \"name\", \"paperCount\", \"citationCount\"])\n",
    "        \n",
    "        # Output to CSV - authorId, name, paper count, and citation count.\n",
    "        distData.select(distData.authorId, distData.name, distData.paperCount, distData.citationCount).write.csv(full_output_path)\n",
    "        \n",
    "        print (\"<Author Nodes> successfully processed.\\nFile output at:\\t\", full_output_path )\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        print (\"An error has occured while processing <Author Nodes>.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 57.02197265625,
      "end_time": 1558784554275.654
     }
    }
   },
   "outputs": [],
   "source": [
    "AzStorage_input_path  = \"wasbs://mag-2019-03-22@ics5114mag.blob.core.windows.net/mag/\"\n",
    "AzStorage_input_filename = \"Authors.txt\"\n",
    "#AzStorage_input_path  = \"/home/data/input/\"\n",
    "#AzStorage_input_filename = \"Authors_sample_10.txt\"\n",
    "AzStorage_input_location = AzStorage_input_path + AzStorage_input_filename\n",
    "\n",
    "\n",
    "AzStorage_output_path   = \"wasbs://parsed-csv-files@ics5114mag.blob.core.windows.net/results/\"\n",
    "#AzStorage_output_path   = \"/home/data/results/\"\n",
    "AzStorage_output_folder = \"authors\"\n",
    "AzStorage_output_location = AzStorage_output_path + AzStorage_output_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 758.85986328125,
      "end_time": 1558784602940.162
     }
    }
   },
   "outputs": [],
   "source": [
    "distData = _1_load_data(AzStorage_input_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 214581.85888671875,
      "end_time": 1558784820043.184
     }
    }
   },
   "outputs": [],
   "source": [
    "_2_author_nodes(distData, AzStorage_output_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Journal-ConferenceNodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Journal\n",
    "# DATASET --> { 0:JournalId, 1:Rank, 2:NormalizedName, 3:DisplayName, 4:Issn, 5:Publisher\n",
    "#               6:Webpage, 7:PaperCount, 8:CitationCount, 9:CreatedDate, \n",
    "#\n",
    "#\n",
    "## Conference Instance\n",
    "# DATASET --> { 0:ConferenceInstanceId, 1:NormalizedName, 2:DisplayName, 3:ConferenceSeriesId, 4:Location\n",
    "#               5:OfficialURL, 6:StartDate, 7:EndDate, 8:AbstractRegistrationDate, 9:SubmissionDeadlineDate, \n",
    "#               10:NotificationDueDate, 11:FinalVersionDueDate, 12:PaperCount, 13:CitationCount, 14:Latitude,\n",
    "#               15:Longitude, 16:CreatedDate }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 39.535888671875,
      "end_time": 1558784934358.591
     }
    }
   },
   "outputs": [],
   "source": [
    "def _2_journal_nodes (journalData, full_output_path_journal):\n",
    "\n",
    "    try:\n",
    "    \n",
    "        # Split columns (tab delimited), filter required columns\n",
    "        journalData = journalData.map(lambda x: x.split(\"\\t\")).map(lambda x: ( x[0], x[2] )) \n",
    "\n",
    "        # Convert to DataFrame\n",
    "        journalData = journalData.toDF([\"journalId\", \"name\" ])\n",
    "        \n",
    "        # Output to CSV - authorId, name, paper count, and citation count.\n",
    "        journalData.select(journalData.journalId, journalData.name).write.csv(full_output_path_journal)\n",
    "        \n",
    "        print (\"<Journal Nodes> successfully processed.\\nFile output at:\\t\", full_output_path_journal )\n",
    "        \n",
    "        # Clear DataFrame\n",
    "        journalData.unpersist()\n",
    "\n",
    "    except:\n",
    "        \n",
    "        print (\"An ERROR has occured while processing <Journal Nodes>.\")\n",
    "        \n",
    "def _3_conf_nodes (confData, full_output_path_conf):\n",
    "\n",
    "    try:\n",
    "    \n",
    "        # Split columns (tab delimited), filter required columns\n",
    "        confData = confData.map(lambda x: x.split(\"\\t\")).map(lambda x: ( x[0], x[1], x[4] )) \n",
    "\n",
    "        # Convert to DataFrame\n",
    "        confData = confData.toDF([\"conferenceId\", \"name\", \"year\" ])\n",
    "        \n",
    "        # Output to CSV - authorId, name, paper count, and citation count.\n",
    "        confData.select(confData.conferenceId, confData.name, confData.year).write.csv(full_output_path_conf)\n",
    "        \n",
    "        print (\"<Conference Instance Nodes> successfully processed.\\nFile output at:\\t\", full_output_path_conf )\n",
    "        \n",
    "        # Clear DataFrame\n",
    "        confData.unpersist()\n",
    "\n",
    "    except:\n",
    "        \n",
    "        print (\"An ERROR has occured while processing <Conference Instance Nodes>.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 48.2822265625,
      "end_time": 1558784936880.188
     }
    }
   },
   "outputs": [],
   "source": [
    "AzStorage_input_path  = \"wasbs://mag-2019-03-22@ics5114mag.blob.core.windows.net/mag/\"\n",
    "AzStorage_input_filename_journals = \"Journals.txt\"\n",
    "#AzStorage_input_path  = \"/home/data/input/\"\n",
    "#AzStorage_input_filename_journals  = \"Journals_sample_10.txt\"\n",
    "AzStorage_input_location_journals = AzStorage_input_path + AzStorage_input_filename_journals\n",
    "\n",
    "#AzStorage_input_filename_conf = \"ConferenceInstances.txt\"\n",
    "AzStorage_input_filename_conf = \"ConferenceInstances_sample_10.txt\"\n",
    "AzStorage_input_location_conf = AzStorage_input_path + AzStorage_input_filename_conf\n",
    "\n",
    "AzStorage_output_path   = \"wasbs://parsed-csv-files@ics5114mag.blob.core.windows.net/results/\"\n",
    "#AzStorage_output_path   = \"/home/data/results/\"\n",
    "\n",
    "AzStorage_output_folder_conf   = \"conferenceinstance\"\n",
    "AzStorage_output_location_conf = AzStorage_output_path + AzStorage_output_folder_conf\n",
    "\n",
    "AzStorage_output_folder_journal = \"journals\"\n",
    "AzStorage_output_location_journal = AzStorage_output_path + AzStorage_output_folder_journal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 242.491943359375,
      "end_time": 1558784938088.699
     }
    }
   },
   "outputs": [],
   "source": [
    "# Load Journal Data\n",
    "journalData = _1_load_data(AzStorage_input_location_journals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 2280.280029296875,
      "end_time": 1558784941383.991
     }
    }
   },
   "outputs": [],
   "source": [
    "# Extract Journal Node\n",
    "_2_journal_nodes (journalData, AzStorage_output_location_journal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 242.823974609375,
      "end_time": 1558784963007.908
     }
    }
   },
   "outputs": [],
   "source": [
    "# Load ConferenceInstance Data\n",
    "confData = _1_load_data(AzStorage_input_location_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 27403.339111328125,
      "end_time": 1558784992094.219
     }
    }
   },
   "outputs": [],
   "source": [
    "# Extract ConferenceInstance Node\n",
    "_3_conf_nodes (confData, AzStorage_output_location_conf )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PaperNodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET --> { 0:PaperId, 1:Rank, 2:Doi, 3:DocType, 4:PaperTitle, 5:OriginalTitle\n",
    "#               6:BookTitle, 7:Year, 8:Date, 9:Publisher, 10:JournalID, 11:ConferenceSeriesId,\n",
    "#               12:ConferenceInstanceId, 13:Volume, 14:Issue, 15:FirstPage, 16:LastPage,\n",
    "#               17:ReferenceCount, 18:CitationCount, 19:EstimatedCitation, 20:OriginalVenue,\n",
    "#               21:CreatedDate }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 56.10693359375,
      "end_time": 1558793004038.278
     }
    }
   },
   "outputs": [],
   "source": [
    "def _2_paper_nodes(distData, full_output_path):\n",
    "\n",
    "    try:\n",
    "        \n",
    "        # Split columns (tab delimited), filter required columns\n",
    "        distData = distData.map(lambda x: x.split(\"\\t\")).map(lambda x: ( (x[0]), (x[3]), (x[5]), (x[7]) , (x[9]), (x[10]), (x[12]) )) \n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        distData = distData.toDF([\"paperId\", \"type\", \"title\", \"year\", \"publisher\", \"journalId\", \"conferenceInstanceId\", ])\n",
    "        \n",
    "        # Add \"unkown\" type for records with empty type field.\n",
    "        distData = distData.withColumn('type', when(distData.type == \"\", \"Unknown\").otherwise(col(\"type\")))\n",
    "        \n",
    "        # Output to CSV - authorId, name, paper count, and citation count.\n",
    "        distData.select(distData.paperId, distData.type, distData.title, distData.year).write.csv(full_output_path)\n",
    "        \n",
    "        print (\"<Paper Nodes> successfully processed.\\nFile output at:\\t\", full_output_path )\n",
    "        \n",
    "        # Remove unwanted Data\n",
    "        return distData.select(distData.paperId, distData.publisher, distData.journalId, distData.conferenceInstanceId)       \n",
    "        \n",
    "    except:\n",
    "        \n",
    "        print (\"An ERROR has occured while processing <Paper Nodes>.\")\n",
    "        \n",
    "def _3_publishers(distData, full_output_path, full_output_path_rel):\n",
    "\n",
    "    try:\n",
    "        \n",
    "        # Create new dataframe from publisher column and drop duplicate names\n",
    "        pubData = distData.select(distData.paperId, distData.publisher).dropDuplicates([\"publisher\"])\n",
    "        \n",
    "        # Assign an id to each institution\n",
    "        pubData = pubData.withColumn(\"publisherId\", monotonically_increasing_id())\n",
    "        \n",
    "        # Output to CSV - Publisher nodes: publisherId, and publisher Name\n",
    "        pubData.select(pubData.publisherId, pubData.publisher).write.csv(full_output_path)\n",
    "        \n",
    "        print (\"<Publisher Nodes> successfully processed.\\nFile output at:\\t\", full_output_path )\n",
    "        \n",
    "        # inner join and use publisher, to create paperid to publisher id relationship\n",
    "        df1 = distData.selectExpr(\"paperId as paperId\", \"publisher as publisher1\").alias(\"df1\")\n",
    "        df2 = pubData.selectExpr (\"publisher as publisher2\", \"publisherId as publisherId\").alias(\"df2\")\n",
    "        \n",
    "        pubRelData = df1.join(df2, col(\"df1.publisher1\") == col(\"df2.publisher2\"), 'inner')\n",
    "        \n",
    "        # Output to CSV - Publisher Relationships: publisherId, and publisher Name\n",
    "        pubRelData.select(pubRelData.paperId, pubRelData.publisherId).write.csv(full_output_path_rel)\n",
    "        \n",
    "        print (\"\\n<Publisher Relationships> successfully processed.\\nFile output at:\\t\", full_output_path_rel )\n",
    "        \n",
    "        # Clear DataFrame\n",
    "        pubData.unpersist()\n",
    "               \n",
    "    except:\n",
    "        \n",
    "        print (\"An ERROR has occured while processing <Publisher Nodes>.\")\n",
    "        \n",
    "def _4_conf_journal(distData, full_output_path_conference, full_output_path_journal):\n",
    "\n",
    "    try:\n",
    "        \n",
    "        # Remove unwanted Columns\n",
    "        distData = distData.select(distData.paperId, distData.journalId, distData.conferenceInstanceId)\n",
    "        \n",
    "        # Output to CSV - ConferenceInstanceId to PaperId Relationship -  Drop entries with no ConferenceInstanceId \n",
    "        distData.select(distData.paperId, distData.conferenceInstanceId).filter(distData.conferenceInstanceId != \"\").write.csv(full_output_path_conference)\n",
    "        \n",
    "        print (\"<ConferenceInstance to Paper Relationships> successfully processed.\\nFile output at:\\t\", full_output_path_conference )\n",
    "               \n",
    "        # Output to CSV - JournalId to PaperId Relationship - Drop entries with no journalId \n",
    "        distData.select(distData.paperId, distData.journalId).filter(distData.journalId != \"\").write.csv(full_output_path_journal)\n",
    "        \n",
    "        print (\"\\n<Journal to Paper Relationships> successfully processed.\\nFile output at:\\t\", full_output_path_journal )\n",
    "        \n",
    "        # Clear DataFrame\n",
    "        distData.unpersist()\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        print (\"An ERROR has occured while processing <ConferenceInstance or Journal to Paper Relationships>.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 53.864013671875,
      "end_time": 1558792967858.482
     }
    }
   },
   "outputs": [],
   "source": [
    "# Azure Storage Locations for input/output of Spark scripts.\n",
    "\n",
    "AzStorage_input_path  = \"wasbs://mag-2019-03-22@ics5114mag.blob.core.windows.net/mag/\"\n",
    "AzStorage_input_filename = \"Papers.txt\"\n",
    "#AzStorage_input_path  = \"/home/data/input/\"\n",
    "#AzStorage_input_filename = \"Papers_sample_10.txt\"\n",
    "AzStorage_input_location = AzStorage_input_path + AzStorage_input_filename\n",
    "\n",
    "\n",
    "AzStorage_output_path    = \"wasbs://parsed-csv-files@ics5114mag.blob.core.windows.net/results/\"\n",
    "#AzStorage_output_path   = \"/home/data/results/\"\n",
    "\n",
    "AzStorage_output_folder_papers = \"papers\"\n",
    "AzStorage_output_location_papers = AzStorage_output_path + AzStorage_output_folder_papers\n",
    "\n",
    "AzStorage_output_folder_publishers = \"publishers\"\n",
    "AzStorage_output_location_publishers = AzStorage_output_path + AzStorage_output_folder_publishers\n",
    "\n",
    "AzStorage_output_folder_publishersRel = \"paper-publisher-rel\"\n",
    "AzStorage_output_location_publishersRel = AzStorage_output_path + AzStorage_output_folder_publishersRel\n",
    "\n",
    "AzStorage_output_folder_confRel   = \"paper-confinstance-rel\"\n",
    "AzStorage_output_location_confRel = AzStorage_output_path + AzStorage_output_folder_confRel\n",
    "\n",
    "AzStorage_output_folder_journalRel = \"paper-journal-rel\"\n",
    "AzStorage_output_location_journalRel = AzStorage_output_path + AzStorage_output_folder_journalRel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 890.6728515625,
      "end_time": 1558792972095.573
     }
    }
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "distData = _1_load_data(AzStorage_input_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 5313.093017578125,
      "end_time": 1558793012713.334
     }
    }
   },
   "outputs": [],
   "source": [
    "# Extract Paper Nodes\n",
    "distData = _2_paper_nodes(distData, AzStorage_output_location_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 648912.3349609375,
      "end_time": 1558793671007.128
     }
    }
   },
   "outputs": [],
   "source": [
    "# Extract Publisher Nodes, and Publisher-to-Paper Relationships\n",
    "_3_publishers(distData, AzStorage_output_location_publishers, AzStorage_output_location_publishersRel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 546234.080078125,
      "end_time": 1558786704411.62
     }
    }
   },
   "outputs": [],
   "source": [
    "# Extract ConferenceInstance-to-Paper Relationships, and Journal-to-Paper Relationships.\n",
    "_4_conf_journal(distData, AzStorage_output_location_confRel, AzStorage_output_location_journalRel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keywords Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 36.947021484375,
      "end_time": 1558818455664.196
     }
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.functions import udf, explode, lower, log, trim, regexp_replace\n",
    "from pyspark.sql.types import *\n",
    "import json\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, col\n",
    "import pyspark.sql.functions as func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 13334.32177734375,
      "end_time": 1558818556981.157
     }
    }
   },
   "outputs": [],
   "source": [
    "# Load text file\n",
    "file = sc.textFile(\"wasbs://mag-2019-03-22@ics5114mag.blob.core.windows.net/nlp/PaperAbstractsInvertedIndex.txt\")\n",
    "#file = sc.textFile(\"/home/data/input/PaperAbstractsInvertedIndex_sample_10.txt\")\n",
    "#fileRDD = file.flatMap(lambda k: k.split(\"\\\\r\\\\n\")).map(lambda k: k.split(\"\\\\t\"))\n",
    "\n",
    "#Split tabbed data into columns\n",
    "fileRDD = file.map(lambda k: k.split(\"\\t\"))\n",
    "\n",
    "#Load into Dataframe\n",
    "fileToDf = fileRDD.toDF([\"Id\",\"JsonData\"])\n",
    "\n",
    "\n",
    "# Define textfile json schema\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField('IndexLength', IntegerType(), True),\n",
    "        StructField('InvertedIndex',  MapType(StringType(), ArrayType(IntegerType())), True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Parse Json and split into columns\n",
    "DF = fileToDf.withColumn(\"JsonData\", from_json(\"JsonData\", schema)).select(col('Id'), col('JsonData.*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 41.60986328125,
      "end_time": 1558818466662.54
     }
    }
   },
   "outputs": [],
   "source": [
    "# Total number of papers in dataset\n",
    "totalNumDocuments = fileToDf.count()\n",
    "totalNumDocuments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 763.056884765625,
      "end_time": 1558818561107
     }
    }
   },
   "outputs": [],
   "source": [
    "# Explode dictionary values into multiple rows (one per keyword)\n",
    "# Remove non alphabetical characters\n",
    "# Set all keywords as lowercase\n",
    "\n",
    "newDF = DF.select(\"Id\", \"IndexLength\", explode(\"InvertedIndex\").alias(\"Keyword\", \"Frequency\")).withColumn(\"Frequency\", size('Frequency'))\\\n",
    ".withColumn(\"Keyword\", trim(regexp_replace(col('Keyword'),'[^A-Za-z ]+', ''))).where(col('Keyword') != \"\")\\\n",
    ".withColumn(\"Keyword\", lower(col('Keyword')))\n",
    "\n",
    "# Calculate Term Frequency\n",
    "newDF = newDF.withColumn(\"TF\", newDF.Frequency/newDF.IndexLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 246.799072265625,
      "end_time": 1558818579463.175
     }
    }
   },
   "outputs": [],
   "source": [
    "# Determine frequency of keyword across all papers\n",
    "idf_DF = newDF.groupby('Keyword').count().select(col(\"Keyword\").alias(\"Keyword\"), col(\"count\").alias(\"Count\"))#.sort(col(\"Count\").desc())\n",
    "\n",
    "# Calcualte Inverse Document Frequency\n",
    "idf_DF = idf_DF.withColumn(\"IDF\", log(totalNumDocuments/idf_DF.Count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 246.119873046875,
      "end_time": 1558818580959.422
     }
    }
   },
   "outputs": [],
   "source": [
    "# Join TF and IDF results by keyword\n",
    "left_join = newDF.alias('a').join(idf_DF.alias('b'), newDF.Keyword == idf_DF.Keyword, how='left_outer') # Could also use 'left_outer'\n",
    "left_join = left_join.withColumn(\"TFIDF\", func.round(left_join.TF*left_join.IDF, 4)).select(col(\"Id\"), col(\"a.Keyword\"), col(\"TFIDF\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 7034423.260986328,
      "end_time": 1558825646409.861
     }
    }
   },
   "outputs": [],
   "source": [
    "# Create window for each Id, rank by TFIDF score, and select top 5 Keywords\n",
    "window = Window.partitionBy(left_join['Id']).orderBy(left_join['TFIDF'].desc())\n",
    "\n",
    "top_5 = left_join.select('*', rank().over(window).alias('rank')).filter(col('rank') <= 5).drop('rank')\n",
    "\n",
    "top_5.write.csv(\"wasbs://parsed-csv-files@ics5114mag.blob.core.windows.net/results/keywords\")\n",
    "#top_5.write.csv(\"/home/data/results/keywords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
